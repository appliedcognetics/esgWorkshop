{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0be6baed-e8d4-472e-90fc-c5ba94ae8e62","showTitle":false,"title":""}},"source":["# Lab N+1 OpenAI Processing\n","## Overview \n","One of the challenges of ESG documents is that the data in the documents is unstructured and oft important data is contained in infographics. This notebook demonstrates how to leverage OpenAI to perform classifications, summarization and extract structured data from the target document.\n","\n","This lab will demonstrate how to break down PDF documents into various elements\n","* **Pages**\n","  1. Classify what data is contained on the page.\n","  1. Effectively locate the data you want to extract eg. Find the page which has the CEO statement, Find Tables of data \n","  1. Summarize the information on the page\n","* **Images**\n","  1. Identify Images a Custom Vision model trained on UN SDG images \n","* **Links**\n","  1. Use links in the document to build relationships to other relevant documents for additional processing\n","\n","## Process\n","The document(s) will be broken down into individual pages, each page will be stored then various example pipelines of processing will be applied to the documents. There will be examples of how to store the extracted data and associate it with the original document , then store the data in a data store of your choosing.\n","  \n","## Prerequisites\n","1. Ensure that the [DataBricks Setup](https://github.com/appliedcognetics/esgWorkshop/blob/main/src/workshop/documents/part_databricks.md) of the lab was completed.\n","1. After completing the **DataBricks Setup** the folowing libraries will be installed on your cluster.\n","    * PyMUPDF\n","    * SynapseML"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b39317ad-9a86-4dd1-a17f-00f4406e0fc0","showTitle":false,"title":""}},"source":["#### Step 1 : Mount Lab Storage Account Read Account to your Workspace\n","Mount the lab storage account to access the lab data. Simply run this cell and the lab storage account containing PDF's will be mounted to your account.\n","\n","If you receive `Error: Storage Account Not Mounted` the directory has not mounted properly, please go back and review your configuration.\n","\n","### Notes\n","\n","1. The following notebook should connect to the keyvault if properly configure to get the keys.If we cant connect to the key vault, create your own storage account and get the sas key for the storage container \n","1. You will still get a storage mounted if the SAS Key is wrong, check connectivity by running `ls -a  /dbfs/mnt/{read_mount_name}/apollo`. *Add advanced checking of storage connectivty*"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a78e2148-daf0-49ac-80b0-e2e5f370cab1","showTitle":false,"title":""}},"source":["# Secrets\n","1. Setup secret scopes [Databricks KeyVault secret scopes](https://learn.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes)\n","1. Ensure you have access to secrets\n","2. After running the cell you should see secrets\n","\n"," \n","labsecrets should be our scope name"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"636f745c-e2d7-4f4f-b979-824d3d19478a","showTitle":false,"title":""}},"outputs":[],"source":["# Test Secrets\n","dbutils.secrets.get(scope=\"labsecrets\", key=\"read-sas-key\")\n","print(a)\n","dbutils.secrets.list(\"labsecrets\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ed872a55-ec14-4730-bf1a-667765f3df57","showTitle":false,"title":""}},"outputs":[],"source":["import os\n","\n","##Use the LAB storage account provided . Do Not Change the information in this cell\n","lab_storage_account = \"synapse20221212adls\" \n","# sas_key = ## Use this SAS Key from the lab \n","sas_key =dbutils.secrets.get(scope=\"labsecrets\", key=\"read-sas-key\")\n","container = 'pdf-data' ## Container with the PDF data\n","folder = '<folder-name>'  ## Not used\n","mount_name = 'thanksV'  ## Mount Name Here \n","lab_pdf_directory = f\"/dbfs/mnt/{mount_name}\"\n","# Setup config required for Mounting\n","configs = {f\"fs.azure.sas.{container}.{lab_storage_account}.blob.core.windows.net\": sas_key}\n","\n","configs = {f\"fs.azure.sas.{container}.{lab_storage_account}.blob.core.windows.net\": sas_key}\n","\n","\n","                    \n","# Mount the blob storage\n","if not (os.path.exists(lab_pdf_directory)):\n","    dbutils.fs.mount(\n","        source = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/\",\n","        mount_point = f\"/mnt/{mount_name}\",\n","        extra_configs = configs)\n","\n","# Check for proper mounting of the directory\n","if  os.path.exists(lab_pdf_directory):\n","    print('Success ! Storage Account Mounted Properly')\n","else :\n","    print('Error: Storage Account Not Mounted')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1b24018a-e247-4c74-8b8b-79a2c07008fa","showTitle":false,"title":""}},"outputs":[],"source":["import os\n","\n","##Use the LAB storage account provided . Do Not Change the information in this cell\n","lab_storage_account = \"synapse20221212adls\" \n","## Refactor to get the key from keyvault.\n","# sas_key = ## Use this SAS Key from the lab \n","\n","read_sas_key =dbutils.secrets.get(scope=\"labsecrets\", key=\"read-sas-key\")\n","\n","container = 'pdf-data' ## Container with the PDF data\n","folder = '<folder-name>'  ## Not used\n","read_mount_name = 'pdf-data-source'  ## Mount Name Here \n","\n","lab_pdf_directory = f\"/dbfs/mnt/{read_mount_name}\"\n","# Setup configs required for Mounting\n","configs = {f\"fs.azure.sas.{container}.{lab_storage_account}.blob.core.windows.net\": read_sas_key}\n","# Mount the blob storage\n","\n","# If you get an error run the unmount command in the next cell\n","dbutils.fs.mount(\n","    source = f\"wasbs://{container}@{lab_storage_account}.blob.core.windows.net/\",\n","    mount_point = f\"/mnt/{read_mount_name}\",\n","    extra_configs = configs)\n","\n","# Check for proper mounting of the directory\n","dbutils.fs.ls(f\"dbfs:/mnt/{read_mount_name}\")\n","    \n","    # Command for performing a ls on the directory\n","#dbutils.fs.ls(f\"/mnt/pdf-data/apollo\")\n","#dbutils.fs.ls (lab_pdf_directory)\n","# This is the Unmount Command if you need to unmount the blob storaged\n","#dbutils.fs.unmount(f\"/mnt/{read_mount_name}\")\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"01d93a75-72be-4d8f-b762-c910b03938fb","showTitle":false,"title":""}},"source":["### DO NOT RUN UNLESS YOU GOT AN ALREADY MOUNTED ERROR in the previous cell\n","This next cell is for debugging unmounting examples"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"be6b681a-e3ad-4bab-b5ce-889f58a9cab4","showTitle":false,"title":""}},"outputs":[],"source":["#dbutils.fs.ls(f\"dbfs:/mnt/pdf-data-source\")\n","#dbutils.fs.unmount(f\"/mnt/pdf_data_source\")\n","#dbutils.fs.ls(f\"/mnt/pdf-data/apollo\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3fe19690-0e98-4282-ba21-b9c6911d2d5c","showTitle":false,"title":""}},"source":["### For Debugging Storage Connections\n","Check for a directory listing\n","**Perform an ```ls``` on the {read_mount_name}** to check for connectivity"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5f68a83e-1408-4844-81d3-1c951d6fdccd","showTitle":false,"title":""}},"outputs":[],"source":["%sh\n","ls -a  /dbfs/mnt/pdf_data_source/apollo\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0275f8a1-4956-4795-850f-1b6b14d229da","showTitle":false,"title":""}},"source":["#### Step 2 : Mount your storage container to Workspace\n","In this step you will connect your storage container for read/write access\n","You will be using your own storage account in this example\n","\n","### Prerequisite \n","  1. Setup a container called `pdf-write-data` in your lab storage account. [Lab Directions Here ](https://github.com/appliedcognetics/esgWorkshop/blob/main/src/workshop/documents/part_0.md#create-the-following-containers-in-azure-storage)\n","  2. Create a SAS token for the **container** `pdf-write-data` with the appropriate permissions (read,list,add,write)."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"30557609-cd83-4101-9597-c3832075c398","showTitle":false,"title":""}},"outputs":[],"source":["# Create a Storage Account for writing PDF Data  and get the Storage Account Container Key\n","storage_account = \"synapse20221212adls\" ##<replace with your storage account name>\n","write_sas_key=dbutils.secrets.get(scope=\"labsecrets\", key=\"write-sas-key\")\n","#Needs write permission\n","container = 'pdf-write-data' ## Do not change\n","folder = '<folder-name>'\n","write_mount_name = 'pdf-write-data'\n","\n","# Setep config required for Mounting\n","configs = {f\"fs.azure.sas.{container}.{storage_account}.blob.core.windows.net\": write_sas_key}\n","# Mount the blob storage\n","dbutils.fs.mount(\n","    source = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/\",\n","    mount_point = f\"/mnt/{write_mount_name}\",\n","    extra_configs = configs)\n","\n","dbutils.fs.ls(f\"dbfs:/mnt/{write_mount_name}\")\n","\n","# View all the Mounts\n","#dbutils.fs.mounts()\n","# Unmount\n","#dbutils.fs.unmount(f\"/mnt/{mount_name}\")\n","\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"89fca5bf-1edf-4faa-ba51-41769f8b4ca1","showTitle":false,"title":""}},"outputs":[],"source":["# Debbugging for unmounting \n","#dbutils.fs.unmount(f\"/mnt/pdf-write-data\")\n","# Debugging for connection \n","#dbutils.fs.ls(f\"dbfs:/mnt/pdf-write-data\")\n","\n","\n","#dbutils.fs.ls(f\"/dbfs/mnt/{write_mount_name}/other\")\n","print(os.path.exists(f\"/dbfs/mnt/{write_mount_name}/other/two\"))\n","if not os.path.exists(f\"/dbfs/mnt/{write_mount_name}/other\"):\n","        dbutils.fs.mkdirs(f\"dbfs:/mnt/{write_mount_name}/other\")\n","        print(\"created\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4e9eed2d-b3d0-4e55-80fa-b2d35bb38c79","showTitle":false,"title":""}},"outputs":[],"source":["%sh\n","ls -a  /dbfs/mnt/pdf-write-data/other"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5cf72006-41aa-4c57-b8de-b3d0a95640a2","showTitle":false,"title":""}},"source":["## Visit the directory in your write mount container to see the individual pdf pages\n","\n","1. Extract the images from the document and store\n","1. Extact the links from the document"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f8667cec-985b-47a7-bcbc-2bf323665915","showTitle":false,"title":""}},"source":["## Hack Challenge\n","1. Mount your own storage account and read the  documents"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"244ecc74-9c7f-4972-9495-6db367f63e31","showTitle":false,"title":""}},"source":["#OpenAI Examples"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6e882972-1d83-455b-a2fc-3befe7eff427","showTitle":false,"title":""}},"outputs":[],"source":["import fitz\n","import io\n","from PIL import Image\n","\n","\n","# We can try different ESG reports here using Apollo to Test\n","base_directory= f\"dbfs:/mnt/{write_mount_name}/\"\n","input_directory=\"apollo\"\n","outputdirectory = f\"{input_directory}/output/pages\"\n","rawtext =f\"{base_directory}{input_directory}/output/rawtext/\"\n","images =f\"{input_directory}/output/images/\"\n","src = fitz.open(f\"/dbfs/mnt/{read_mount_name}/{input_directory}/apollo-2021-esg-report-final.pdf\")\n","\n","\n","width, height = fitz.paper_size(\"a4\")  # A4 portrait output page format\n","r = fitz.Rect(0, 0, width, height)\n","\n","#Check for directories\n","if not os.path.exists(f\"/dbfs/mnt/{write_mount_name}/{outputdirectory}\"):\n","        dbutils.fs.mkdirs(f\"dbfs:/mnt/{write_mount_name}/{outputdirectory}\")    \n","\n","if not os.path.exists(f\"/dbfs/mnt/{write_mount_name}/{images}\"):\n","        dbutils.fs.mkdirs(f\"{rawtext}\")    \n","\n","if not os.path.exists(f\"/dbfs/mnt/{write_mount_name}/{images}\"):\n","        dbutils.fs.mkdirs(f\"dbfs:/mnt/{write_mount_name}/{images}\")    \n","\n","text_list=[]\n","# Change to Enumerate \n","#l=0\n","for l,spage in enumerate(src):\n","    #create a new document for each page\n","    doc = fitz.open() \n","    #l = l+1\n","    # create new output page\n","    page = doc.new_page(-1,\n","                      width = width,\n","                      height = height)\n","    # insert input page into the correct rectangle\n","    page.show_pdf_page(r,  # select output rect\n","                     src,  # input document\n","                     spage.number)  # input page number\n","    #print(spage.get_text())\n","    #text_list.append (\"page\":l,\"text\":page.get_text()})\n","    text_list.append ([l,page.get_text()])\n","# Write the data to the output directory\n","    print(f\" writing text ...{rawtext}page-{l}\")\n","    #Write the ext\n","    dbutils.fs.put(f\"{rawtext}page-{l}\" ,spage.get_text(),True)\n","    doc.save(f\"/dbfs/mnt/{write_mount_name}/{outputdirectory}page-{l}\".format(ls=l) + '.pdf', garbage=3, deflate=True)\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3945ee7a-650a-4212-b894-3838b4a52021","showTitle":false,"title":""}},"outputs":[],"source":["import pandas as pd\n","#Create a dataframe from the text in the document \n","\n","pandf= pd.DataFrame (text_list,columns=[\"id\",\"text\"] )\n","\n","#Append a prompt to the content\n","#prompt=\"Please summarize the text above into a sentence of no more than 20 wordss.\"\n","prompt=\"if the text above has more than 100 words please summarize the text in 20 words or less, is the text above has less that 100 words please classify the text into one of the items in the following list [Title Page, Table of Contents]\"\n","\n","\n","pandf[\"prompt\"] = \"###Content##\\n\"+pandf[\"text\"]+\"###\\n\\n\"+prompt\n","\n","df2 = spark.createDataFrame(pandf)\n","\n","promptdf= df2.select(\"prompt\")\n","\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3da00401-24b1-41aa-86a2-b1e2b29a7aaa","showTitle":false,"title":""}},"source":["### Example for Page 6 of Apollo \n","```prompt_page6=\"Please extract the total number of employees and the number of employees by region from the text, the number of employees in each region must be less that the total number of employees\"```"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"00e9eb3e-99f3-4753-9201-7610d05a8288","showTitle":false,"title":""}},"outputs":[],"source":["df2.show()\n","#df2.select (col(\"Content\")).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"dcce54f1-c65a-49d7-bec7-afc17691e4be","showTitle":false,"title":""}},"outputs":[],"source":["import synapse.ml\n","from synapse.ml.cognitive import *\n","from pyspark.sql.functions import col\n","from synapse.ml.cognitive import OpenAICompletion\n","from pyspark.sql.functions import col\n","\n","# Not used currently \n","\n","read_sas_key =dbutils.secrets.get(scope=\"labsecrets\", key=\"cognitive-services-key\")\n","\n","cognitive_service_name = \"<Your linked service for text analytics>\"\n","cognitive_services_key=dbutils.secrets.get(scope=\"labsecrets\", key=\"cognitive-services-key\")\n","cognitive_services_region = \"eastus\"\n","\n","#openAI US the Lab Provided version \n","api_type = \"azure\"\n","api_base = \"https://openai202211.openai.azure.com/\"\n","api_servicename =\"openai202211\"\n","api_version = \"2022-12-01\"\n","api_key = dbutils.secrets.get(scope=\"labsecrets\", key=\"openai-key\")\n","engine=\"text-davinci-002\"\n","\n","# Docs on Completion API https://mmlspark.blob.core.windows.net/docs/0.10.0/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.OpenAICompletion\n","\n","#Define the completion object \n","completion = (\n","    OpenAICompletion()\n","    .setSubscriptionKey(api_key)\n","    .setDeploymentName(engine)\n","    .setUrl(\"https://{}.openai.azure.com/\".format(api_servicename))\n","    .setMaxTokens(2000)\n","    .setPromptCol(\"prompt\")\n","    .setErrorCol(\"error\")\n","    .setOutputCol(\"completions\")\n","    .setTopP(1.0)\n","    #.setStop(None)\n","    \n",")\n","\n","completed_df = completion.transform(promptdf).cache()\n","\n","\n","#completed_df = completion.transform(df).cache()\n","\n","#Show the text\n","display(\n","    completed_df.select(\n","        col(\"prompt\"),\n","        col(\"error\"),\n","        col(\"completions.choices.text\").getItem(0).alias(\"text\"),\n","    )\n",")\n","\n","#Show the json \n","display(\n","    completed_df.select(\n","        col(\"prompt\"),\n","        col(\"error\"),\n","        col(\"completions.choices\"),\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"92acaaf9-5e27-43d9-93be-cc88e7648cd7","showTitle":false,"title":""}},"source":["```\n","#SAMPLES\n","\n","\n","\n","# df = spark.createDataFrame(\n","#     [\n","#         (\"A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more if the star was especially metal-rich.[1] Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical white holes, quark stars, and strange stars.[2] Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4 solar masses.[3] They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core past white dwarf star density to that of atomic nuclei.\\n\\nTl;dr\",),\n","#     ]\n","# ).toDF(\"prompt\")\n","\n","\"\"\"\n","# This one works\n","df = spark.createDataFrame(\n","    [\n","        (\"Classify the following news article into 1 of the following categories: categories: [Business, Tech, Politics, Sport, Entertainment]\\n\\nnews article: Donna Steffensen Is Cooking Up a New Kind of Perfection. The Internetâ€™s most beloved cooking guru has a buzzy new book and a fresh new perspective:\\n\\nClassified category:\",),\n","    ]\n",").toDF(\"prompt\")\n","\"\"\"\n","\n","df = spark.createDataFrame(\n","    [\n","        (\"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\\n\\nPlease make a table summarizing the fruits from Goocrux\\n| Fruit | Color | Flavor |\\n| Neoskizzles | Purple | Sweet |\\n| Loheckles | Grayish blue | Tart |\",),\n","    ]\n",").toDF(\"prompt\")\n","```"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"48d71f9e-eb9f-4628-802d-a566eb3e1e69","showTitle":false,"title":""}},"source":["# Lab \n","## Step 0\n","\n","1. Install the SynapseML Packages into the Cluster for the machine [See Instructions](https://docs.databricks.com/libraries/cluster-libraries.html). Also check the lab setup [Instructions here](https://github.com/appliedcognetics/esgWorkshop/blob/main/src/workshop/documents/part_databricks.md)\n","\n","    1. Go to the Compute >> (Select the Cluster Name) >> Libraries >> Install New >> (Select Maven) >> Enter Maven Coordinates\n","    2. Enter the Azure Cognitive Services Key \n","    3. Enter the Azure Search Key, Service Name and Search Index"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"122646d5-80e8-40fe-9317-812abf7fc204","showTitle":false,"title":""}},"outputs":[],"source":["import os\n","from pyspark.sql.functions import udf, trim, split, explode, col, monotonically_increasing_id, lit\n","from pyspark.sql.types import StringType\n","from synapse.ml.core.spark import FluentAPI\n","\n","#vision is the name of the conitive services account where this key came from \n","cognitive_services_key = \"\"\n","cognitive_services_region = \"eastus\"\n","\n","# search_service = \"searchservicenas\"\n","# search_key = \"WbzZ7OEcD0s50ICWgVbeT7HHZjRWVbEMYjwQLe9ro1AzSeB8Jn3Z\"\n","# search_index = \"azureblob-index-n2\""]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"790d959a-ed27-410b-b0a4-bbf9248f6d7c","showTitle":false,"title":""}},"source":["# LAB\n","## Step 1\n","\n"," 1. Enter the name of the storage account \n"," 2. Run cell to grab all of the names of the PDF files we will be processing\n"," 3. The names of all of the PDF should be displayed in the output, if you configured this section correctly"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e38b2eb6-afc3-4b35-a859-e29a133feeb6","showTitle":false,"title":""}},"outputs":[],"source":["def blob_to_url(blob):\n","    [prefix, postfix] = blob.split(\"@\")\n","    container = prefix.split(\"/\")[-1]\n","    split_postfix = postfix.split(\"/\")\n","    account = split_postfix[0]\n","    filepath = \"/\".join(split_postfix[1:])\n","    return \"https://{}/{}/{}\".format(account, container, filepath)\n","\n","storage_account_name = \"Enter Name of Storage Account Here\"\n","\n","df2 = (spark.read.format(\"binaryFile\")\n","    .load(\"wasbs://ignite2021@mmlsparkdemo.blob.core.windows.net/form_subset/*\")\n","    .select(\"path\")\n","    .limit(10)\n","    .select(udf(blob_to_url, StringType())(\"path\").alias(\"url\"))\n","    .cache())\n","    \n","display(df2)\n","\n","\n","dict3 = [{'url':'https://synapse20221212adls.blob.core.windows.net/pdf-data/apollo/pages/4up-17abc.pdf?sp=rl&st=2023-02-08T14:30:36Z&se=2023-02-08T22:30:36Z&spr=https&sv=2021-06-08&sr=c&sig=Q7tpR5JR1zUz8%2F0nCA3wP61NLaEXObsLuHyWyptcElU%3D'},\n","         {'url': 'https://github.com/Azure/azure-sdk-for-python/raw/main/sdk/formrecognizer/azure-ai-formrecognizer/samples/sample_forms/forms/Invoice_1.pdf'}\n","        ]\n","\n","df3 =spark.createDataFrame(dict3)\n","display(df3)\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0ea19819-c043-4fce-b542-c6fdad8fb406","showTitle":false,"title":""}},"source":["# LAB\n","## Step 2 : Analyze the Documents\n","\n","This section is an example of using Cognitive Services to Analyze the Document to extract the text and tables from the document\n","\n","Try the different sections to demonstrate the output of different Azure Cogntive Services\n","(Documentation Link Here to D)\n","\n","# Note: Modify this section to use the form reader to analyze the document and show example of other services that can be integrated with OpenAI\n","\n","  1. Analyze the Documents"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"18e90e56-c4b8-4027-a2ad-36d8c48c684d","showTitle":false,"title":""}},"outputs":[],"source":["from synapse.ml.cognitive import AnalyzeInvoices\n","\n","analyzed_df = (AnalyzeInvoices()\n","    .setSubscriptionKey(cognitive_services_key)\n","    .setLocation(cognitive_services_region)\n","    .setImageUrlCol(\"url\")\n","    .setOutputCol(\"invoices\")\n","    .setErrorCol(\"errors\")\n","    .setConcurrency(5)\n","    .transform(df3)\n","    .cache())\n","\n","display(analyzed_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"db377b8a-dcc9-47b8-b12d-f5d896ed6034","showTitle":false,"title":""}},"outputs":[],"source":["# Fix the error with FormOntology Reader"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"28fb044d-65e6-4b97-a7ee-d0092331b6c4","showTitle":false,"title":""}},"outputs":[],"source":["from synapse.ml.cognitive import FormOntologyLearner\n","\n","itemized_df = (FormOntologyLearner()\n","    .setInputCol(\"invoices\")\n","    .setOutputCol(\"extracted\")\n","    .fit(analyzed_df)\n","    .transform(analyzed_df)\n","    .select(\"url\", \"extracted\")\n","              )\n","\n","display(itemized_df)\n","\n","#.select(\"url\", \"extracted.*\").select(\"*\", explode(col(\"Items\")).alias(\"Item\"))\n","#   .drop(\"Items\").select(\"Item.*\", \"*\").drop(\"Item\")\n","#"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cb6c4b76-93d3-4656-9d63-72cc1098ccd0","showTitle":false,"title":""}},"outputs":[],"source":["# Classify Each page with OpenAI\n","    - Add the code for the business classifications\n","    - Interate through the pages and add the classifications to the data game\n","# Classify the Entire Document"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2e4d01cc-383e-47f7-9013-eb6f45637e96","showTitle":false,"title":""}},"outputs":[],"source":["## Load the GRI Rules into a DataFrame\n","Add the code for this\n","Load Sample GRI Rules\n","Interate through each page and identify and extract the GRI metrics from each page"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"48885d64-500c-4be3-9ce8-39258233d651","showTitle":false,"title":""}},"outputs":[],"source":["## Extract Structured Data from InfoGraphics\n","Extract Structure Data from Infographics Add the example here"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9e79be8d-cafc-40f5-a766-6be42145a36a","showTitle":false,"title":""}},"outputs":[],"source":["## Use Compter Vision to Identify SDG Icons \n","Add Code Here "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bcbae040-7869-421c-a584-7bc039ba1fb0","showTitle":false,"title":""}},"outputs":[],"source":["## Summarize Page Text here\n","Allow the users to create new prompts to show examples of OpenAI doing Summarization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d5de9c48-7e44-40b8-a950-1fa6d12ebf39","showTitle":false,"title":""}},"outputs":[],"source":["from synapse.ml.cognitive import Translate\n","\n","translated_df = (Translate()\n","    .setSubscriptionKey(cognitive_services_key)\n","    .setLocation(cognitive_services_region)\n","    .setTextCol(\"Description\")\n","    .setErrorCol(\"TranslationError\")\n","    .setOutputCol(\"output\")\n","    .setToLanguage([\"zh-Hans\",\"it\", \"fr\", \"ru\", \"cy\"])\n","    .setConcurrency(5)\n","    .transform(itemized_df)\n","    .withColumn(\"Translations\", col(\"output.translations\")[0])\n","    .drop(\"output\", \"TranslationError\")\n","    .cache())\n","\n","display(translated_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c6372bda-b61f-40d4-a6b7-aad2163f1251","showTitle":false,"title":""}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"60daf8c5-26c6-4373-8062-4c730b8b4369","showTitle":false,"title":""}},"source":["# Lab Step 10\n","\n","\n","## Explaination \n","  Add an DocID to the dataframe so that the dataframe can be written to Azure Cognitive Search Index. This will make the data available in the interface and searchable in the index\n","\n","## Objective of the Lab\n","  Write the dataframe to the SQL Database so that the data appears in the search filters"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"93c847a7-fd07-4690-b3de-01b120010c58","showTitle":false,"title":""}},"outputs":[],"source":["from synapse.ml.cognitive import *\n","\n","s_df=(translated_df.withColumn(\"DocID\", monotonically_increasing_id().cast(\"string\"))\n","        .withColumn(\"SearchAction\", lit(\"upload\")))\n","      \n"," #       .writeToAzureSearch(\n"," #            subscriptionKey=search_key,\n"," #            actionCol=\"SearchAction\",\n"," #            serviceName=search_service,\n"," #            indexName=search_index,\n"," #            keyCol=\"DocID\"\n"," #   ))\n","\n","AzureSearchWriter.writeToAzureSearch(\n","    df=s_df,\n","    subscriptionKey=search_key,\n","    actionCol=\"SearchAction\",\n","    serviceName=search_service,\n","    indexName=search_index,\n","    keyCol=\"DocID\")\n","                                     \n","  \n","display(s_df)\n","\n","# Add a Step to write the data to the SQL database\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":606245514930853,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"Lab n+1","notebookOrigID":420836378019836,"widgets":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"b5b2bc34872e55c33479cb5300fa7c6dd7d158a895efaf3c5b1a789cdefb1130"}}},"nbformat":4,"nbformat_minor":0}
